{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc5a88a-b244-4f94-a583-820b930fa2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb96d8c6-63a4-47e8-9fac-0b7770981ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9adc52e-0ff3-4063-b64b-2880da62afcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.formatCheck.enabled</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.formatCheck.enabled",
         "false"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.formatCheck.enabled=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594dcd69-142e-4104-84d2-65f62b64552d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12acc47946a5-pin') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('index', IntegerType(), True),\n",
    "        StructField('unique_id', StringType(), True),\n",
    "        StructField('title', StringType(), True),\n",
    "        StructField('description', StringType(), True),\n",
    "        StructField('poster_name', StringType(), True),\n",
    "        StructField('follower_count', IntegerType(), True),\n",
    "        StructField('tag_list', StringType(), True),\n",
    "        StructField('is_image_or_video', StringType(), True),\n",
    "        StructField('image_src', StringType(), True),\n",
    "        StructField('downloaded', BooleanType(), True),\n",
    "        StructField('save_location', StringType(), True),\n",
    "        StructField('category', StringType(), True)    ]\n",
    ")\n",
    "\n",
    "df_pin = df_pin.withColumn(\"data\", from_json(\"data\", schema)).select(col('data.*'))\n",
    "\n",
    "#display(df_pin)\n",
    "\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12acc47946a5-geo') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('ind', IntegerType(), True),\n",
    "        StructField('timestamp', TimestampType(), True),\n",
    "        StructField('latitude', FloatType(), True),\n",
    "        StructField('longitude', FloatType(), True),\n",
    "        StructField('country', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_geo = df_geo.withColumn(\"data\", from_json(\"data\", schema)).select(col('data.*'))\n",
    "\n",
    "#display(df_geo)\n",
    "\n",
    "\n",
    "\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName','streaming-12acc47946a5-user') \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('ind', IntegerType(), True),\n",
    "        StructField('first_name', StringType(), True),\n",
    "        StructField('last_name', StringType(), True),\n",
    "        StructField('age', IntegerType(), True),\n",
    "        StructField('date_joined', TimestampType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_user = df_user.withColumn(\"data\", from_json(\"data\", schema)).select(col('data.*'))\n",
    "\n",
    "#display(df_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef4e922-2157-4980-8a8e-d0bcd3c2bbe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[66]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f218cc831c0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[66]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7f218cc831c0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cleaned_df_pin = df_pin.replace({'No description available Story format': None}, subset=['description'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'User Info Error': None}, subset=['follower_count'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'Image src error.': None}, subset=['image_src'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'User Info Error': None}, subset=['poster_name'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e': None}, subset=['tag_list'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'No Title Data Available': None}, subset=['title'])\n",
    "\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'k': '000'}, subset=['follower_count'])\n",
    "cleaned_df_pin = cleaned_df_pin.replace({'m': '000000'}, subset=['follower_count'])\n",
    "\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "                   \n",
    "\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"follower_count\", cleaned_df_pin[\"follower_count\"].cast(\"int\"))\n",
    "cleaned_df_pin.dtypes\n",
    "\n",
    "\n",
    "cleaned_df_pin = cleaned_df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "\n",
    "cleaned_df_pin = cleaned_df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "cleaned_df_pin = cleaned_df_pin.select(\"ind\", \"unique_id\", \"title\",\"description\",\"follower_count\",\"poster_name\",\"tag_list\",\"is_image_or_video\",\"image_src\",\"save_location\",\"category\")\n",
    "display(cleaned_df_pin)\n",
    "\n",
    "\n",
    "cleaned_df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12acc47946a5_pin_table\")\n",
    "\n",
    "\n",
    "geo_df_clean =  df_geo.withColumn(\"coordinates\", concat(\"latitude\", lit(\",\"), \"longitude\"))\n",
    "geo_df_clean = geo_df_clean.withColumn(\"timestamp\", geo_df_clean[\"timestamp\"].cast(\"timestamp\"))\n",
    "geo_df_clean.dtypes\n",
    "\n",
    "geo_df_clean = geo_df_clean.select(\"ind\",\"country\",\"coordinates\",\"timestamp\")\n",
    "\n",
    "display(geo_df_clean)\n",
    "\n",
    "geo_df_clean.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12acc47946a5_geo_table\")\n",
    "\n",
    "user_df_clean = df_user.withColumn(\"user_name\", concat(\"first_name\", lit(\" \"), \"last_name\"))\n",
    "user_df_clean = user_df_clean.withColumn(\"date_joined\", user_df_clean[\"date_joined\"].cast(\"timestamp\"))\n",
    "user_df_clean = user_df_clean.select(\"ind\",\"user_name\",\"age\",\"date_joined\")\n",
    "\n",
    "\n",
    "user_df_clean.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(\"12acc47946a5_user_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15990f62-481b-4e8d-8ce2-8048e422b26a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1143679294347555,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kinesis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
