# pinterest-data-pipeline project
## In this project, a similar system to pinterest is being built using both historical and real-time Pinterest post data using scripts emulating pinterest user activity.
Installation instructions
Usage instructions
File structure of the project
License information

### A description of the project: what it does, the aim of the project, and what you learned

Provided with a script emulating data generated by pinterest users. The script provides an RDS DB connection to tables with data resembleing data received by the Pinterest API when a POST request is made by a user uploading data to pinterest and randomly picks a random "post"/row every few seconds to emulate user activity. 

## Kafka
Configured an amazon EC2 instance to use as an Apache Kafka client machine. Created three topics, one for each set of data to be collected.

## AWS MSK
Connected an MSK cluster to an S3 bucket so that any data going through the cluster are automatically saved and stored in a dedicated S3 bucket.

## AWS API Gateway
Configured an API, which will send data to the MSK cluster, which in turn will be stored in an S3 bucket.

## Databricks
Set up a Databricks account, mounted an S3 bucket to Databricks and read data from AWS into Databricks. Using Spark, performed data cleaning and computations on Databricks.

## Airflow
Used AWS Managed Workflows with Apache Airflow (MWAA) to orchestrate Databricks Workloads. In order to automate the batch processing process, a file containing a Directed Acyclic Graph (DAG) was uploaded to the MWAA environment so that the Databricks notebook is scheduled to run daily.

## Kinesis
Write streaming data to Delta tables. The data are streamed to Databricks using Kinesis Data Streams through an API with Kinesis proxy integration.
